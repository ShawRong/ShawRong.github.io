<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="How sparsity is employed in Acceleration
When multiplying by zero or very small values, we can avoid the computation entirely.
If 90% percent of values are zero, we can theoretically only need to compute 10% of the operations.
The hardware like GPU support this acceleration in hardware way.
It can:

Reduce memory bandwidth. (no load zero values)
Fewer arithmetic operations. (skip some multiplications and addition)
Lower energy consumption. (based on previous 2 optimization)

Three types of Sparsity in LLM
Weight Sparsity: Zeros in model parameters. These weights pruned during training or post-training.">  

  <title>
    
      Sparsity
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.51652302d3a998bf7887aed5c2cf89141bbebdf45a2c8f87b0717a3cf4f51c4e53c694c328fb1de78c3a625a1c01f80745bf1f2f42c040647a245cbbb6c2d1d7.css" integrity="sha512-UWUjAtOpmL94h67Vws&#43;JFBu&#43;vfRaLI&#43;HsHF6PPT1HE5TxpTDKPsd54w6YlocAfgHRb8fL0LAQGR6JFy7tsLR1w==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
                <div class="post-meta">
                    <a href="/">..</a>

                    <p>
                        <time datetime="2025-07-04 14:16:29.661 &#43;0000 UTC">
                            2025-07-04
                        </time>
                    </p>
                </div>

<article>
    <h1>Sparsity</h1>

    

    <h1 id="how-sparsity-is-employed-in-acceleration">How sparsity is employed in Acceleration</h1>
<p>When multiplying by zero or very small values, we can avoid the computation entirely.</p>
<p>If 90% percent of values are zero, we can theoretically only need to compute 10% of the operations.</p>
<p>The hardware like GPU support this acceleration in hardware way.</p>
<p>It can:</p>
<ul>
<li>Reduce memory bandwidth. (no load zero values)</li>
<li>Fewer arithmetic operations. (skip some multiplications and addition)</li>
<li>Lower energy consumption. (based on previous 2 optimization)</li>
</ul>
<h1 id="three-types-of-sparsity-in-llm">Three types of Sparsity in LLM</h1>
<p><strong>Weight Sparsity</strong>: Zeros in model parameters. These weights pruned during <strong>training or post-training</strong>.</p>
<p><strong>Activation Sparsity</strong>: Zero in intermediate activations during forward pass, typically from <strong>RELU-like</strong> functions that output zero for negative inputs. <em>This sparsity is input-dependent and changes dynamically based on the data being processed.</em></p>
<p><strong>Attention Sparsity</strong>: Zeros or near-zeros in attention weight matrices. Many attention heads <strong>focus on only a subset of tokens</strong>, creating natural sparsity patterns. <em>This is also input-dependent and varies across different sequences</em></p>
<h1 id="randomunstructured-and-structured-pattern">Random(Unstructured) and Structured Pattern</h1>
<p><strong>Random(Unstructured) Sparsity</strong>: zero values appear scattered throughout the tensor without particular pattern.
<strong>Drawback</strong>:</p>
<ul>
<li>Irregular memory access patterns</li>
<li>Hard to vectorize operations</li>
<li>Requires complex indexing schemes</li>
</ul>
<p><strong>Structured Pattern</strong>: zero values follow regular patterns like entire rows, columns, or blocks being zero (and 2:4).</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Regular memory access patterns</li>
<li>Easier to map to hardware parallelism</li>
</ul>
<h1 id="how-structured-pattern-come-out">How Structured Pattern Come Out</h1>
<p><strong>Training-Time Methods</strong>:
Structured <strong>Pruning</strong> During Training:</p>
<ul>
<li>Block-wise pruning: remove entire blocks of weights</li>
<li>Channel pruning: Remove entire channels/filters in convolutional layer or attention heads</li>
<li>N:M sparsity: For every M consecutive weights, exactly N are forced to zero (e.g. 2:4)</li>
</ul>
<p><strong>Regularization</strong> with structure constraints: add penalty terms to the loss function that encourage structured patterns:</p>
<ul>
<li>Group LASSO regularization to zero out entire groups of weights</li>
<li>Structured dropout that follows the desired sparsity pattern</li>
</ul>
<p><strong>Post-Training Methods</strong>:
Structured <strong>Pruning</strong>: Take a dense pre-trained model and apply structured pruning:</p>
<ul>
<li>Magnitude-based: Within each block/group, keep only the largest weights and zero the rest</li>
<li>Gradient-based: Use gradient information to decide which structure to prune</li>
<li>Fisher information: Use second-order information to make more informed pruning decisions</li>
</ul>
<p>Knowledge Distillation: Train a sparse student model with structured constraints to mimic a dense teacher model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># code of 2:4 sparsity, how did it derived</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply_2_4_sparsity</span>(weights):
</span></span><span style="display:flex;"><span>	<span style="color:#75715e">#Reshape to group of 4</span>
</span></span><span style="display:flex;"><span>	groups <span style="color:#f92672">=</span> weights<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># Find 2 smallest magnitude weights</span>
</span></span><span style="display:flex;"><span>	indices <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argsort(np<span style="color:#f92672">.</span>abs(group))[:<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># Zero them out</span>
</span></span><span style="display:flex;"><span>	group[indices] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> weights<span style="color:#f92672">.</span>reshape(original_shape)
</span></span></code></pre></div>
</article>

            </div>
        </main>
    </body></html>
