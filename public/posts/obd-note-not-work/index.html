<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="Note of Optimal Brain Damage
interests

Complexity measures: Vapnik-Chervonenkis dimensionality, a time-honored(albeit inexact) measure of complexity: simply the number of non-zero free parameters.
some measure of network complexity: in the statistical inference literature and NN literature.
How can the author make statement that: automatic network minimization procedure ands as as an interactive tool to suggest better architectures.
One of the main points of this paper is to move beyond the approximation that magnitude equanls saliency
I don&rsquo;t think it works now.

Key findings:

use weight decay, non-proportionate for sparsity or mixture precision
It omit the cross term, lacking ability of find redundant pattern, and not simpler than magnitude way.

Questions:

what&rsquo;s group lasso, which one is better compared with weight decay? why it&rsquo;s suggested weight decay is not suitable for sparsity? can weight decay therefore be used for mixture precision.
does optimal brain damage really works? we need to try or research. there are several modern method:

Magnitude-based pruning (simpler, often similarly effective)
Gradual pruning during training
Lottery ticket hypothesis approaches
More sophisticated second-order methods like Fisher Information


what&rsquo;s fisher information?
In the assumption of OBD, it says: &ldquo;delta E caused by deleting several parameters is the sum of the delta E&rsquo;s caused by deleting each parameter individually.&rdquo; Does this assumption really work?

claude talk
-claude public
-OBD does not work today.">  

  <title>
    
      OBD Note (not work)
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.51652302d3a998bf7887aed5c2cf89141bbebdf45a2c8f87b0717a3cf4f51c4e53c694c328fb1de78c3a625a1c01f80745bf1f2f42c040647a245cbbb6c2d1d7.css" integrity="sha512-UWUjAtOpmL94h67Vws&#43;JFBu&#43;vfRaLI&#43;HsHF6PPT1HE5TxpTDKPsd54w6YlocAfgHRb8fL0LAQGR6JFy7tsLR1w==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
                <div class="post-meta">
                    <a href="/">..</a>

                    <p>
                        <time datetime="2025-07-16 02:55:21.308 &#43;0000 UTC">
                            2025-07-16
                        </time>
                    </p>
                </div>

<article>
    <h1>OBD Note (not work)</h1>

    

    <h1 id="note-of-optimal-brain-damage">Note of Optimal Brain Damage</h1>
<p><strong>interests</strong></p>
<ul>
<li>Complexity measures: <strong>Vapnik-Chervonenkis dimensionality</strong>, a time-honored(albeit inexact) measure of complexity: simply the number of non-zero free parameters.</li>
<li>some measure of network complexity: in the statistical inference literature and NN literature.</li>
<li>How can the author make statement that: automatic network minimization procedure ands as <strong>as an interactive tool to suggest better architectures.</strong></li>
<li>One of the main points of this paper is to <strong>move beyond the approximation that magnitude equanls saliency</strong></li>
<li>I don&rsquo;t think it works now.</li>
</ul>
<h1 id="key-findings">Key findings:</h1>
<ul>
<li>use weight decay, non-proportionate for sparsity or mixture precision</li>
<li>It omit the cross term, lacking ability of find redundant pattern, and not simpler than magnitude way.</li>
</ul>
<h1 id="questions">Questions:</h1>
<ul>
<li>what&rsquo;s group lasso, which one is better compared with weight decay? why it&rsquo;s suggested weight decay is not suitable for sparsity? can weight decay therefore be used for mixture precision.</li>
<li>does optimal brain damage really works? we need to try or research. there are several modern method:
<ul>
<li>Magnitude-based pruning (simpler, often similarly effective)</li>
<li>Gradual pruning during training</li>
<li>Lottery ticket hypothesis approaches</li>
<li>More sophisticated second-order methods like Fisher Information</li>
</ul>
</li>
<li>what&rsquo;s fisher information?</li>
<li>In the assumption of OBD, it says: &ldquo;delta E caused by deleting several parameters is the sum of the delta E&rsquo;s caused by deleting each parameter individually.&rdquo; Does this assumption really work?</li>
</ul>
<h1 id="claude-talk">claude talk</h1>
<p>-<a href="https://claude.ai/public/artifacts/6bc3dc4c-53b7-4548-a9cb-12d1533f3baf">claude public</a>
-<a href="https://claude.ai/public/artifacts/6535b7e0-a73b-49ea-a8e0-4010974251d4">OBD does not work today.</a></p>

</article>

            </div>
        </main>
    </body></html>
