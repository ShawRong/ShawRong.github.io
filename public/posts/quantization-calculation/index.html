<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="Symmetric Quantization
suppose we get data in range [r_min, r_max].
and suppose we get &lsquo;b&rsquo; bits to quant.
we can get scale by using r_max, only
b = 4 #suppose int4
quant = (2^(b-1)) - 1 # how many positive number in int4
scale = r_max / quant

x = 10
x&#39; = round(x / scale)
$$
\Delta = \frac {\text{max}}{2^{b - 1} - 1}
$$
$$
x&rsquo; = \text{round}(\frac{x}{\Delta})
$$
$$
x = x&rsquo;\Delta
$$
Asymmetric Quantization
we need r_min and r_max to calculate the scale, and we need to shift zero point to the middle to make:">  

  <title>
    
      Quantization Calculation
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.51652302d3a998bf7887aed5c2cf89141bbebdf45a2c8f87b0717a3cf4f51c4e53c694c328fb1de78c3a625a1c01f80745bf1f2f42c040647a245cbbb6c2d1d7.css" integrity="sha512-UWUjAtOpmL94h67Vws&#43;JFBu&#43;vfRaLI&#43;HsHF6PPT1HE5TxpTDKPsd54w6YlocAfgHRb8fL0LAQGR6JFy7tsLR1w==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
                <div class="post-meta">
                    <a href="/">..</a>

                    <p>
                        <time datetime="2025-07-23 15:08:37.198 &#43;0000 UTC">
                            2025-07-23
                        </time>
                    </p>
                </div>

<article>
    <h1>Quantization Calculation</h1>

    

    <h1 id="symmetric-quantization">Symmetric Quantization</h1>
<p>suppose we get data in range [r_min, r_max].</p>
<p>and suppose we get &lsquo;b&rsquo; bits to quant.</p>
<p>we can get scale by using r_max, only</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>b <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span> <span style="color:#75715e">#suppose int4</span>
</span></span><span style="display:flex;"><span>quant <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span><span style="color:#f92672">^</span>(b<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#75715e"># how many positive number in int4</span>
</span></span><span style="display:flex;"><span>scale <span style="color:#f92672">=</span> r_max <span style="color:#f92672">/</span> quant
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>x<span style="color:#e6db74">&#39; = round(x / scale)</span>
</span></span></code></pre></div><p>$$
\Delta = \frac {\text{max}}{2^{b - 1} - 1}
$$
$$
x&rsquo; = \text{round}(\frac{x}{\Delta})
$$
$$
x = x&rsquo;\Delta
$$</p>
<h1 id="asymmetric-quantization">Asymmetric Quantization</h1>
<p>we need r_min and r_max to calculate the scale, and we need to shift zero point to the middle to make:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>scale <span style="color:#f92672">*</span> (q_min <span style="color:#f92672">-</span> zero_point) <span style="color:#f92672">=</span> r_min
</span></span><span style="display:flex;"><span>r_min <span style="color:#f92672">/</span> scale <span style="color:#f92672">=</span> q_min <span style="color:#f92672">-</span> zero_point
</span></span><span style="display:flex;"><span>zero_point <span style="color:#f92672">=</span> q_min <span style="color:#f92672">-</span> r_min <span style="color:#f92672">/</span> scale
</span></span><span style="display:flex;"><span><span style="color:#75715e">#we know, q_min = -2^(b-1)</span>
</span></span><span style="display:flex;"><span>zero_point <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">^</span>(b<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> round(r_min <span style="color:#f92672">/</span> scale)
</span></span></code></pre></div><p>$$
\Delta = \frac{\text{max} - \text{min}}{2^{b} - 1 }
$$
$$
z = -2^{b-1} + \text{round}(\frac {\text{min}}{\Delta})
$$
$$
x&rsquo; = \text{round}(\frac{x}{\Delta} + z)
$$
$$
x = \Delta * (x&rsquo; - z)
$$</p>
<h1 id="gguf-quant">GGUF Quant.</h1>
<p>GGUF(Group-wise Quantization).
divide matrix into <strong>block</strong>, and we do quant. to individual block. each block are equipped with a scale.</p>
<p>default: group in <strong>row</strong>, each 32 elements is a group .</p>
<h1 id="gptq">GPTQ</h1>
<h2 id="optimal-brain-surgeon">Optimal Brain Surgeon</h2>
<p>for a object function (loss function), L, we can do taylor expansion.
$$
L(\mathbf{w}) = L(\mathbf{w}_0) + g^T(\mathbf{w} - \mathbf{w}_0) + \frac{1}{2}(\mathbf{w} - \mathbf{w}_0)^T\mathbf{H}(\mathbf{w} - \mathbf{w}_0) + o(|\mathbf{w} - \mathbf{w}_0|^3)
$$
here g stand for gradient and H stand for hessian matrix.</p>
<p>we can assume g is 0, since we achieve a local minimum.</p>
<p>and we denote w - w_0 as \delta w
$$
\Delta \mathbf{w} = \mathbf{w} - \mathbf{w}_0
$$
we can get, delta L is:
$$
\Delta L = \frac{1}{2} \Delta \mathbf{w}^T \mathbf{H} \Delta \mathbf{w}
$$
suppose we update, w_i to 0, which is pruning, we can get a formula:
$$
\Delta w_i + w_i = 0
$$
which is: (e_i stand for one-hot vector at index i)
$$
\mathbf{e}_i^T \Delta \mathbf{w} + w_i = 0
$$
we can get our optimization problem with constraint, at each step when we want to prune w_i. (use lagrange).</p>
<p>$$
\min_{\Delta \mathbf{w}, i} \frac{1}{2} \Delta \mathbf{w}^T \mathbf{H} \Delta \mathbf{w} + \lambda(\mathbf{e}_i^T\Delta\mathbf{w} + w_i)
$$
$$
\frac{\partial}{\partial \Delta \mathbf{w}} \left[ \frac{1}{2} \Delta\mathbf{w}^T \mathbf{H} \Delta\mathbf{w} + \lambda (\mathbf{e}_i^T \Delta \mathbf{w} + w_i) \right] = 0</p>
<p>$$
gives
$$
\mathbf{H} \Delta \mathbf{w} + \lambda \mathbf{e}_i = 0
$$
$$
\Delta \mathbf{w}^* = -\lambda \mathbf{H}^{-1} \mathbf{e}<em>i
$$
gives
$$
\frac {\partial L}{\partial \lambda} = \mathbf{e}<em>i \Delta \mathbf{w} + w_i = 0
$$
i.e.
$$
\lambda^* = \frac{w_i}{(\mathbf{H}^{-1})</em>{ii}}
$$
$$
\Delta \mathbf{w} = - \frac{w_i}{(\mathbf{H}^{-1})</em>{ii}} \mathbf{H}^{-1}\mathbf{e}_i
$$
so, we should update after each prune, like:
w_i &lt;- w_i + \Delta w^*</p>
<h2 id="gradient-parallel-surgeon">Gradient Parallel Surgeon</h2>
<p>suppose, we want:
here, E is a matrix where each column is a diagonal matrix, with 1 or 0, to keep w_i or make w_i to be 0. And we have constraint like:
$$
\Delta w_i + w_i = 0
$$
$$
\mathbf{E}^T (\Delta \mathbf{w} + \mathbf{w}) = \mathbf{0}
$$
Therefore, we get our lagrange:
$$
L(\Delta \mathbf{w}, \mathbf{\lambda}) = \frac{1}{2} \Delta \mathbf{w}^T \mathbf{H} \Delta \mathbf{w} + \mathbf{\lambda}^T \mathbf{E}^T(\Delta \mathbf{w} + \mathbf{w})
$$
$$
\frac{\partial L}{\partial \Delta \mathbf{w}} =  \mathbf{H} \Delta \mathbf{w} + \mathbf{E} \mathbf{\lambda} = 0
$$
$$
\frac{\partial L}{\partial \mathbf{\lambda}} = \mathbf{E}^T(\Delta \mathbf{w} + \mathbf{w}) = 0
$$
$$
\Delta \mathbf{w}^* = - \mathbf{H}^{-1} \mathbf{E}(\mathbf{E}^T \mathbf{H}^{-1}\mathbf{E})^{-1}\mathbf{w}
$$
<strong>solution 1</strong>: use the diagonal approximation:
$$
\text{diag}(\mathbf{H}) = (\frac{\partial ^2 L}{\partial w_0^2}, &hellip;)
$$
calculation can be very quick.</p>

</article>

            </div>
        </main>
    </body></html>
