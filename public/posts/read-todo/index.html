<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="üìö Priority Reading Queue
1. Weight Noise Injection-Based MLPs With Group Lasso Penalty: Asymptotic Convergence and Application to Node Pruning

Authors: Wang J, Chang Q, Chang Q, Liu Y, Pal NR
Journal: IEEE Transactions on Cybernetics, 2019
Volume/Issue: Vol. 49, No. 12, pp. 4346-4364
DOI: 10.1109/TCYB.2018.2864142
Key Focus:

Shows L2 weight decay inadequacy for sparse solutions
Proposes group lasso as regularizer alternative
Node pruning applications for fault-tolerant MLPs


Status: ‚è≥ To Read
Notes: Key paper showing why traditional weight decay fails for sparsity


2. A Novel Pruning Algorithm for Smoothing Feedforward Neural Networks Based on Group Lasso Method

Authors: Wang J, Xu C, Yang X, Zurada JM
Journal: IEEE Transactions on Neural Networks and Learning Systems
Year: 2018
Volume/Issue: Vol. 29, No. 5, pp. 2012-2024
DOI: 10.1109/TNNLS.2017.2748585
Key Focus:

Four new backpropagation variants using Group Lasso
Smoothing functions to handle non-differentiability
Direct comparison with Weight Decay, Weight Elimination


Status: ‚è≥ To Read
Notes: Comprehensive comparison with traditional weight decay methods


3. Group Sparse Regularization for Deep Neural Networks

Authors: Scardapane S, Comminiello D, Hussain A, Uncini A
Conference/Journal: ArXiv preprint
Year: 2016
ArXiv ID: 1607.00485
Key Focus:

Joint optimization of weights, neuron count, and feature selection
Group Lasso penalty for network connections
Extensive comparison with classical weight decay


Status: ‚è≥ To Read
Notes: Foundational paper on group sparse regularization vs weight decay


üìù Reading Notes Template
For each paper, capture:">  

  <title>
    
      Read TODO
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.51652302d3a998bf7887aed5c2cf89141bbebdf45a2c8f87b0717a3cf4f51c4e53c694c328fb1de78c3a625a1c01f80745bf1f2f42c040647a245cbbb6c2d1d7.css" integrity="sha512-UWUjAtOpmL94h67Vws&#43;JFBu&#43;vfRaLI&#43;HsHF6PPT1HE5TxpTDKPsd54w6YlocAfgHRb8fL0LAQGR6JFy7tsLR1w==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
                <div class="post-meta">
                    <a href="/">..</a>

                    <p>
                        <time datetime="2025-07-16 02:57:14.938 &#43;0000 UTC">
                            2025-07-16
                        </time>
                    </p>
                </div>

<article>
    <h1>Read TODO</h1>

    

    <h2 id="-priority-reading-queue">üìö Priority Reading Queue</h2>
<h3 id="1-weight-noise-injection-based-mlps-with-group-lasso-penalty-asymptotic-convergence-and-application-to-node-pruning">1. Weight Noise Injection-Based MLPs With Group Lasso Penalty: Asymptotic Convergence and Application to Node Pruning</h3>
<ul>
<li><strong>Authors</strong>: Wang J, Chang Q, Chang Q, Liu Y, Pal NR</li>
<li><strong>Journal</strong>: IEEE Transactions on Cybernetics, 2019</li>
<li><strong>Volume/Issue</strong>: Vol. 49, No. 12, pp. 4346-4364</li>
<li><strong>DOI</strong>: 10.1109/TCYB.2018.2864142</li>
<li><strong>Key Focus</strong>:
<ul>
<li>Shows L2 weight decay inadequacy for sparse solutions</li>
<li>Proposes group lasso as regularizer alternative</li>
<li>Node pruning applications for fault-tolerant MLPs</li>
</ul>
</li>
<li><strong>Status</strong>: ‚è≥ To Read</li>
<li><strong>Notes</strong>: <em>Key paper showing why traditional weight decay fails for sparsity</em></li>
</ul>
<hr>
<h3 id="2-a-novel-pruning-algorithm-for-smoothing-feedforward-neural-networks-based-on-group-lasso-method">2. A Novel Pruning Algorithm for Smoothing Feedforward Neural Networks Based on Group Lasso Method</h3>
<ul>
<li><strong>Authors</strong>: Wang J, Xu C, Yang X, Zurada JM</li>
<li><strong>Journal</strong>: IEEE Transactions on Neural Networks and Learning Systems</li>
<li><strong>Year</strong>: 2018</li>
<li><strong>Volume/Issue</strong>: Vol. 29, No. 5, pp. 2012-2024</li>
<li><strong>DOI</strong>: 10.1109/TNNLS.2017.2748585</li>
<li><strong>Key Focus</strong>:
<ul>
<li>Four new backpropagation variants using Group Lasso</li>
<li>Smoothing functions to handle non-differentiability</li>
<li>Direct comparison with Weight Decay, Weight Elimination</li>
</ul>
</li>
<li><strong>Status</strong>: ‚è≥ To Read</li>
<li><strong>Notes</strong>: <em>Comprehensive comparison with traditional weight decay methods</em></li>
</ul>
<hr>
<h3 id="3-group-sparse-regularization-for-deep-neural-networks">3. Group Sparse Regularization for Deep Neural Networks</h3>
<ul>
<li><strong>Authors</strong>: Scardapane S, Comminiello D, Hussain A, Uncini A</li>
<li><strong>Conference/Journal</strong>: ArXiv preprint</li>
<li><strong>Year</strong>: 2016</li>
<li><strong>ArXiv ID</strong>: 1607.00485</li>
<li><strong>Key Focus</strong>:
<ul>
<li>Joint optimization of weights, neuron count, and feature selection</li>
<li>Group Lasso penalty for network connections</li>
<li>Extensive comparison with classical weight decay</li>
</ul>
</li>
<li><strong>Status</strong>: ‚è≥ To Read</li>
<li><strong>Notes</strong>: <em>Foundational paper on group sparse regularization vs weight decay</em></li>
</ul>
<hr>
<h2 id="-reading-notes-template">üìù Reading Notes Template</h2>
<p>For each paper, capture:</p>
<ul>
<li><input disabled="" type="checkbox"> <strong>Main Contribution</strong>: How does it extend/replace weight decay?</li>
<li><input disabled="" type="checkbox"> <strong>Methodology</strong>: What specific regularization technique is proposed?</li>
<li><input disabled="" type="checkbox"> <strong>Experimental Setup</strong>: What baselines are compared?</li>
<li><input disabled="" type="checkbox"> <strong>Key Results</strong>: Performance vs traditional weight decay</li>
<li><input disabled="" type="checkbox"> <strong>Theoretical Insights</strong>: Why does the proposed method work better?</li>
<li><input disabled="" type="checkbox"> <strong>Implementation Details</strong>: Any code or algorithmic specifics</li>
<li><input disabled="" type="checkbox"> <strong>Future Directions</strong>: What questions does this raise?</li>
</ul>
<h2 id="-key-questions-to-address">üîç Key Questions to Address</h2>
<ol>
<li><strong>Fundamental Question</strong>: Why does traditional L2 weight decay fail for structured sparsity?</li>
<li><strong>Methodological</strong>: How do group-based penalties differ from element-wise penalties?</li>
<li><strong>Practical</strong>: What are the computational trade-offs between methods?</li>
<li><strong>Theoretical</strong>: What convergence guarantees exist for these approaches?</li>
</ol>
<h2 id="-completion-checklist">‚úÖ Completion Checklist</h2>
<ul>
<li><input disabled="" type="checkbox"> Paper 1: Weight Noise Injection-Based MLPs</li>
<li><input disabled="" type="checkbox"> Paper 2: Novel Pruning Algorithm for Smoothing</li>
<li><input disabled="" type="checkbox"> Paper 3: Group Sparse Regularization for DNNs</li>
<li><input disabled="" type="checkbox"> Synthesis: Write summary comparing all three approaches</li>
<li><input disabled="" type="checkbox"> Implementation: Try reproducing key results from one paper</li>
</ul>
<hr>
<p><strong>Last Updated</strong>: <em>Add date when you start reading</em><br>
<strong>Priority</strong>: High - Core understanding of weight decay limitations in structured sparsity</p>

</article>

            </div>
        </main>
    </body></html>
